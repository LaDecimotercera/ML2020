\documentclass[a4paper,UTF8]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{color}
\usepackage{ctex} %引入CTeX宏包
\usepackage[unicode=true,colorlinks,linkcolor=blue,citecolor=blue]{hyperref} % 超链接
\setCJKmainfont[BoldFont=SimHei, ItalicFont=KaiTi]{SimSun}
\usepackage{enumerate}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tcolorbox}

\theoremstyle{definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\usepackage{multirow}              

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.5in}
\setlength{\topmargin}{-0.5in}
% \setlength{\textheight}{9.5in}
%%%%%%%%%%%%%%%%%%此处用于设置页眉页脚%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}                                
\usepackage{lastpage}                                           
\usepackage{layout}                                             
\footskip = 12pt 
\pagestyle{fancy}                    % 设置页眉                 
\lhead{2020年春季}                    
\chead{机器学习导论}                                                
% \rhead{第\thepage/\pageref{LastPage}页} 
\rhead{作业四}                                                                                               
\cfoot{\thepage}                                                
\renewcommand{\headrulewidth}{1pt}  			%页眉线宽，设为0可以去页眉线
\setlength{\skip\footins}{0.5cm}    			%脚注与正文的距离           
\renewcommand{\footrulewidth}{0pt}  			%页脚线宽，设为0可以去页脚线

\makeatletter 									%设置双线页眉                                        
\def\headrule{{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
		\hrule\@height 1.0pt \@width\headwidth\vskip1pt	%上面线为1pt粗  
		\hrule\@height 0.5pt\@width\headwidth  			%下面0.5pt粗            
		\vskip-2\headrulewidth\vskip-1pt}      			%两条线的距离1pt        
	\vspace{6mm}}     								%双线与下面正文之间的垂直间距              
\makeatother  

%--

%--
\begin{document}
\title{机器学习导论\\习题四}
\author{181220031, 李惟康, \texttt{liwk@smail.nju.edu.cn}}
\maketitle


\section*{学术诚信}

本课程非常重视学术诚信规范，助教老师和助教同学将不遗余力地维护作业中的学术诚信规范的建立。希望所有选课学生能够对此予以重视。\footnote{参考尹一通老师\href{http://tcs.nju.edu.cn/wiki/}{高级算法课程}中对学术诚信的说明。}

\begin{tcolorbox}
	\begin{enumerate}
		\item[(1)] 允许同学之间的相互讨论，但是{\color{red}\textbf{署你名字的工作必须由你完成}}，不允许直接照搬任何已有的材料，必须独立完成作业的书写过程;
		\item[(2)] 在完成作业过程中，对他人工作（出版物、互联网资料）中文本的直接照搬（包括原文的直接复制粘贴及语句的简单修改等）都将视为剽窃，剽窃者成绩将被取消。{\color{red}\textbf{对于完成作业中有关键作用的公开资料，应予以明显引用}}；
		\item[(3)] 如果发现作业之间高度相似将被判定为互相抄袭行为，{\color{red}\textbf{抄袭和被抄袭双方的成绩都将被取消}}。因此请主动防止自己的作业被他人抄袭。
	\end{enumerate}
\end{tcolorbox}

\section*{作业提交注意事项}
\begin{tcolorbox}
	\begin{enumerate}
		\item[(1)] 请在LaTeX模板中{\color{red}\textbf{第一页填写个人的姓名、学号、邮箱信息}}；
		\item[(2)] 本次作业需提交该pdf文件、问题4可直接运行的源码(main.py)、问题4的输出文件(学号\_ypred.csv)，将以上三个文件压缩成zip文件后上传。zip文件格式为{\color{red}\textbf{学号.zip}}，例如170000001.zip；pdf文件格式为{\color{red}\textbf{学号\_姓名.pdf}}，例如170000001\_张三.pdf。
		\item[(3)] 未按照要求提交作业，或提交作业格式不正确，将会{\color{red}\textbf{被扣除部分作业分数}}；
		\item[(4)] 本次作业提交截止时间为{\color{red}\textbf{5月14日23:59:59}}。除非有特殊情况（如因病缓交），否则截止时间后不接收作业，本次作业记零分。
	\end{enumerate}
\end{tcolorbox}

\newpage

\section*{\textbf{[30 pts]} Problem	 1 [Kernel Functions]}

\begin{enumerate}[(1)]
	\item \textbf{[10 pts]} 对于$\bm{x},\bm{y} \in \mathbb{R}^N$，考虑函数$\kappa(x,y) = \tanh( a \bm{x}^\top \bm{y} + b)$，其中$a,b$是任意实数。试说明$a \geqslant 0,b \geqslant 0$是$\kappa$为核函数的必要条件。
	\item \textbf{[10 pts]} 考虑$ \mathbb{R}^N $上的函数$ \kappa(\bm{x},\bm{y}) = (\bm{x}^\top \bm{y} + c)^d $，其中$c$是任意实数，$d,N$是任意正整数。试分析函数$\kappa$何时是核函数，何时不是核函数，并说明理由。
	\item \textbf{[10 pts]} 当上一小问中的函数是核函数时，考虑$d=2$的情况，此时$\kappa$将$N$维数据映射到了什么空间中？具体的映射函数是什么？更一般的，对$d$不加限制时，$\kappa$将$N$维数据映射到了什么空间中？(本小问的最后一问可以只写结果)
\end{enumerate}

\begin{solution}
~\\(1) $\kappa$是有效的核函数$\Longrightarrow$核函数矩阵$\mathbf{K}$是对称半正定的。($\mathbf{K}$为对称矩阵显然)\\代入函数$\kappa(\bm{x},\bm{y}) = \tanh( a \bm{x}^\top \bm{y} + b),\ a,b \in \mathbb{R}.$  根据半正定矩阵的性质有: $$\kappa(\bm{x},\bm{x}) = \tanh(a\|\bm{x}\|^2+b) \geqslant 0,\ \text{i.e.}\ a\|\bm{x}\|^2+b \geqslant 0\ (\bm{x}\in \mathbb{R}^N)$$ 因此得到: $a\geqslant 0,\ b\geqslant 0$
 \\(2) 设$\bm{n} = (n_0,n_1,\cdots ,n_d)$，其中$n_i$均为非负整数。令$d\choose \bm{n}$为多项式系数，即:$${d \choose \bm{n}} = {d \choose n_0,n_1,\cdots ,n_d} = \frac{d!}{n_0!n_1!\dots n_d!}$$
 利用多项式定理将核函数展开可得:
 \begin{equation}\nonumber
 \begin{aligned}
 \kappa(\bm{x},\bm{y}) &= (\bm{x}^\top \bm{y} + c)^d = \left(\sum_{i=1}^N x_iy_i + c\right)^d = (c + x_1y_1 + x_2y_2 + \dots + x_Ny_N)^d \\&= \sum_{\sum_{i=0}^dn_i} {d \choose \bm{n}} c^{n_0} (x_1y_1)^{n_1} (x_2y_2)^{n_2} \dots (x_dy_d)^{n_d}\\&= \sum_{\sum_{i=0}^dn_i} {d \choose \bm{n}} c^{n_0} \left(\prod_{k=1}^d x_k^{n_k}\right) \left(\prod_{k=1}^d y_k^{n_k}\right)
 \end{aligned} 
 \end{equation}  
 \\由此可见，$c\geqslant 0$时，令$a_{\bm{n}} = {d \choose \bm{n}} c^{n_0}$:
$$\kappa(\bm{x},\bm{y}) = \sum_{\sum_{i=0}^dn_i} \left(\sqrt{a_{\bm{n}}}\prod_{k=1}^d x_k^{n_k}\right) \left(\sqrt{a_{\bm{n}}}\prod_{k=1}^d y_k^{n_k}\right) = \phi(x)^\top\phi(y)$$ 
 故此时$\kappa(\bm{x},\bm{y})$为核函数；反之则不是。
 \\(3) 当$d=2$时，核函数为$\kappa(\bm{x},\bm{y})=(\bm{x}^\top \bm{y} + c)^2.$ 因此有:
	\begin{equation}\nonumber
	\begin{aligned}
	\kappa(\bm{x},\bm{y}) = (\bm{x}^\top \bm{y} + c)^2 &= \left(\sum_{i=1}^N x_iy_i + c\right)^2\\&= \sum_{i=1}^N (x_i^2)(y_i^2) + \sum_{i=2}^N\sum_{j=1}^{i-1} (\sqrt{2}x_ix_j)(\sqrt{2}y_iy_j) + \sum_{i=1}^{N}(\sqrt{2c}x_i)(\sqrt{2c}y_i) + c^2
	\end{aligned}	
	\end{equation}	
	由此可得:
	\begin{equation}\nonumber
	\begin{aligned}	
	\phi(x) = \langle x_N^2,\dots x_1^2, \sqrt{2}x_Nx_{N-1},\dots \sqrt{2}x_Nx_1,  \sqrt{2}x_{N-1}x_{N-2},\dots \sqrt{2}x_{N-1}x_1,\dots \sqrt{2}x_2x_1,\\ \sqrt{2c}x_N,\dots \sqrt{2c}x_1, c\rangle
	\end{aligned}		
	\end{equation}		
即将数据映射到了$N+2 \choose 2$维的特征空间中。\\对$d$不加任何限制时，$\kappa$将数据映射到了$N+d \choose d$维的特征空间中。
\end{solution}

\section*{[30 pts] Problem 2 [Surrogate Function in SVM]}

在软间隔支持向量机问题中，我们的优化目标为
\begin{equation}\label{eq1}
\min _{\boldsymbol{w}, b} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \ell_{0 / 1}\left(y_{i}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b\right)-1\right) . 
\end{equation}
然而$\ell_{0 / 1}$数学性质不太好，它非凸、非连续，使得式(\ref{eq1})难以求解。实践中我们通常会将其替换为“替代损失”，替代损失一般是连续的凸函数，且为$\ell_{0 / 1}$的上界，比如hinge损失，指数损失，对率损失。下面我们证明在一定的条件下，这样的替换可以保证最优解不变。

我们考虑实值函数$h:\mathcal{X}\rightarrow\mathbb{R}$构成的假设空间，其对应的二分类器$f_h:\mathcal{X}\rightarrow\{+1,-1\}$为
$$f_{h}(x)=\left\{\begin{array}{ll}
+1 & \text { if } h(x)\geqslant 0 \\
-1 & \text { if } h(x)<0
\end{array}\right.$$
$h$的期望损失为$R(h)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[I_{f_{h}(x) \neq y}\right]$，其中$I$为指示函数。设$\eta(x)=\mathbb{P}(y=+1|x)$，则贝叶斯最优分类器当$\eta(x)\geqslant \frac{1}{2}$时输出$1$，否则输出$-1$。因此可以定义贝叶斯得分$h^*(x)=\eta(x)-\frac{1}{2}$和贝叶斯误差$R^*=R(h^*)$。

设$\Phi:\mathbb{R}\rightarrow\mathbb{R}$为非减的凸函数且满足$\forall u\in \mathbb{R},1_{u\leqslant 0}\leqslant \Phi(-u)$。对于样本$(x,y)$，定义函数$h$在该样本的$\Phi$-损失为$\Phi(-yh(x))$，则$h$的期望损失为$\mathcal{L}_{\Phi}(h)=\underset{(x, y) \sim \mathcal{D}}{\mathbb{E}}[\Phi(-y h(x))]$。定义$L_{\Phi}(x, u)=\eta(x) \Phi(-u)+(1-\eta(x)) \Phi(u)$，设$h_{\Phi}^{*}(x)=\underset{u \in[-\infty,+\infty]}{\operatorname{argmin}} L_{\Phi}(x, u)$，$\mathcal{L}_{\Phi}^{*}=\mathcal{L}_{\Phi}(h_{\Phi}^{*}(x))$。

我们考虑如下定理的证明：

若对于$\Phi$，存在$s\geqslant 1$和$c>0$满足对$\forall x\in\mathcal{X}$有
\begin{equation}\label{eq2}
\left|h^{*}(x)\right|^{s}=\left|\eta(x)-\frac{1}{2}\right|^{s} \leqslant c^{s}\left[L_{\Phi}(x, 0)-L_{\Phi}\left(x, h_{\Phi}^{*}(x)\right)\right]
\end{equation}
则对于任何假设$h$，有如下不等式成立
\begin{equation}\label{eq3}
R(h)-R^{*} \leqslant 2 c\left[\mathcal{L}_{\Phi}(h)-\mathcal{L}_{\Phi}^{*}\right]^{\frac{1}{s}}
\end{equation}

\begin{enumerate}[(1)]
	\item \textbf{[5 pts]} 请证明
	\begin{equation}\label{eq4}
	\Phi\left(-2 h^{*}(x) h(x)\right)\leqslant L_{\Phi}(x, h(x))
	\end{equation}

	\item \textbf{[10 pts]} 请证明
	\begin{equation}\label{eq5}
	R(h)-R^{*}\leqslant 2 \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left|h^{*}(x)\right| 1_{h(x) h^{*}(x) \leqslant 0}\right]
	\end{equation}
	提示：先证明
	$$R(h)=\underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[2 h^{*}(x) 1_{h(x)<0}+(1-\eta(x))\right]$$
	\item \textbf{[10 pts]} 利用式(\ref{eq4})和式(\ref{eq5})完成定理的证明。
	\item \textbf{[5 pts]} 请验证对于Hinge损失 $\Phi(u)=\max(0,1+u)$，有$s=1,c=\frac{1}{2}$。
\end{enumerate}

\begin{solution}
~\\(1) 由于$\Phi:\mathbb{R}\rightarrow\mathbb{R}$为非减的凸函数，因此有:
$$L_{\Phi}(x, h(x)) = \eta(x) \Phi(-h(x))+(1-\eta(x)) \Phi(h(x)) \geqslant \Phi[(1-2\eta(x))h(x)]$$
注意到: $h^*(x)=\eta(x)-\frac{1}{2}$，那么: $\Phi[(1-2\eta(x))h(x)] = \Phi\left(-2 h^{*}(x) h(x)\right)$。因此:
	$$\Phi\left(-2 h^{*}(x) h(x)\right)\leqslant L_{\Phi}(x, h(x))$$
 (2) 由题可知: 
	\begin{equation}\nonumber	
	\begin{aligned}
	R(h)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[I_{f_{h}(x) \neq y}\right] &= \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\eta(x) 1_{h(x)<0}+(1-\eta(x))1_{h(x)\geqslant 0}\right]\\&=\underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\eta(x) 1_{h(x)<0}+(1-\eta(x))(1-1_{h(x)< 0})\right]\\&=\underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left[2\eta(x)-1\right] 1_{h(x)<0}+(1-\eta(x))\right] \\&= \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[2 h^{*}(x) 1_{h(x)<0}+(1-\eta(x))\right]
	\end{aligned}
	\end{equation}	 
	据此，对于任意的$h$，我们有:			
	\begin{equation}\nonumber	
	\begin{aligned}
	R(h)-R^* &= \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}} \left[2[h^*(x)](1_{h(x)\leqslant 0} - 1_{h^*(x)\leqslant 0})\right] \\&= \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}} \left[2[h^*(x)]\;\text{sgn}(h^*(x))1_{(h(x)h^*(x)\leqslant 0)\land ((h(x),h^*(x))\neq (0,0)}\right] \\&\leqslant 2 \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left|h^{*}(x)\right| 1_{h(x) h^{*}(x) \leqslant 0}\right]
	\end{aligned}
	\end{equation}
	故得证。
 \\(3) 由(5)式可知:			
	\begin{equation}\nonumber	
	\begin{aligned}
	R(h)-R^* \leqslant 2 \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left|h^{*}(x)\right| 1_{h(x) h^{*}(x) \leqslant 0}\right] &= \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left|2\eta(x)-1\right| 1_{h(x) h^{*}(x) \leqslant 0}\right]
	\end{aligned}
	\end{equation}	
	根据Jensen不等式可得:
	$$\underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left|2\eta(x)-1\right| 1_{h(x) h^{*}(x) \leqslant 0}\right]\leqslant \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left|2\eta(x)-1\right|^s 1_{h(x) h^{*}(x) \leqslant 0}\right]^{\frac{1}{s}}$$	
	根据(2)式假设及(4)式有:
	\begin{equation}\nonumber	
	\begin{aligned}	
	\underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left|2\eta(x)-1\right|^s 1_{h(x) h^{*}(x) \leqslant 0}\right]^{\frac{1}{s}} & \leqslant 2c \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left[L_{\Phi}(x, 0)-L_{\Phi}\left(x, h_{\Phi}^{*}(x)\right)\right] 1_{h(x) h^{*}(x) \leqslant 0}\right]^{\frac{1}{s}}\\&\leqslant 2c \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left[\Phi(0)-L_{\Phi}\left(x, h_{\Phi}^{*}(x)\right)\right] 1_{h(x) h^{*}(x) \leqslant 0}\right]^{\frac{1}{s}}\\&\leqslant 2c \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left[\Phi(-2h^*(x)h(x))-L_{\Phi}\left(x, h_{\Phi}^{*}(x)\right)\right] 1_{h(x) h^{*}(x) \leqslant 0}\right]^{\frac{1}{s}}\\&\leqslant 2c \underset{x \sim \mathcal{D}_{x}}{\mathbb{E}}\left[\left[L_{\Phi}\left(x, h(x)\right)-L_{\Phi}\left(x, h_{\Phi}^{*}(x)\right)\right]\right]^{\frac{1}{s}}
	\end{aligned}
	\end{equation}
	故有:
	$$R(h)-R^{*} \leqslant 2 c\left[\mathcal{L}_{\Phi}(h)-\mathcal{L}_{\Phi}^{*}\right]^{\frac{1}{s}}$$
 \\(4) 代入Hinge损失函数$\Phi(u)=\max(0,1+u)$及$s=1,c=\frac{1}{2}$进行验证，即考虑:
 $$\left|\eta(x)-\frac{1}{2}\right| \leqslant \frac{1}{2}\left[L_{\Phi}(x, 0)-L_{\Phi}\left(x, h_{\Phi}^{*}(x)\right)\right]$$ 
 整理得:
 \begin{equation}  
 \left|2\eta(x)-1\right| \leqslant 1-L_{\Phi}\left(x, h_{\Phi}^{*}(x)\right) 
 \end{equation}
 注意到:
 $$L_{\Phi}(x, h_{\Phi}^{*}(x))=\eta(x) \Phi(-h_{\Phi}^{*}(x))+(1-\eta(x)) \Phi(h_{\Phi}^{*}(x))$$
 $$h_{\Phi}^{*}(x)=\underset{u \in[-\infty,+\infty]}{\operatorname{argmin}} L_{\Phi}(x, u)$$
 由题(1)的结论有:
 $$L_{\Phi}(x, u) \geqslant \Phi\left[(1-2\eta(x))u\right]$$
 显然$u \in [-1,1]$时等号成立，此时:$$L_{\Phi}(x, u) = \eta(x)(2-\Phi(u))+(1-\eta(x))\Phi(u)=2\eta(x) + (1-2\eta(x))\Phi(u)$$
 当$2\eta(x)-1>0$时，$h_{\Phi}^*(x) = 1$，代入(6)式验证成立；\\当$2\eta(x)-1\leqslant 0$时，$h_{\Phi}^*(x) = -1$，代入(6)式验证成立；故得证。
\end{solution}

\section*{[20 pts] Problem 3 [Generalization Error of SVM]}

留一损失(leave-one-out error)使用留一法对分类器泛化错误率进行估计，即：每次使用一个样本作为测试集，剩余样本作为训练集，最后对所有测试误差求平均。对于SVM算法$\mathcal{A}$，令$h_S$为该算法在训练集$S$上的输出，则该算法的经验留一损失可形式化定义为
\begin{equation}
	\hat{R}_{\text{LOO}}(\mathcal{A}) = \frac{1}{m} \sum_{i=1}^m 1_{ h_{ S-\{x_i\} } (x_i) \neq y_i } . 
\end{equation}
本题通过探索留一损失的一些数学性质，来分析SVM的泛化误差，并给出一个期望意义下的泛化误差界。(注：本题仅考虑可分情形。)

\begin{enumerate}[(1)]
	\item \textbf{[10pts]} 在实践中，测试误差相比于泛化误差是很容易获取的。虽然测试误差不一定是泛化误差的准确估计，但测试误差与泛化误差往往能在期望意义下一致。试证明留一损失满足该性质，即
	\begin{equation}
		\mathbb{E}_{S \sim \mathcal{D}^m} [ \hat{R}_{\text{LOO} }(\mathcal{A}) ] = \mathbb{E}_{S' \sim \mathcal{D}^{m-1}} [ R(h_{S'}) ] . 
	\end{equation}
	\item \textbf{[5 pts]} SVM之所以取名为SVM，是因为其训练结果仅与一部分样本(即支持向量)有关。这一现象可以抽象的表示为，如果$x$不是$h_S$的支持向量，则$h_{S-\{x\}} = h_S$。这一性质在分析误差时有关键作用，考虑如下问题：如果$x$不是$h_S$的支持向量，$h_{S-\{x\}}$会将$x$正确分类吗，为什么？该问题结论的逆否命题是什么？
	\item \textbf{[5 pts]} 基于上一小问的结果，试证明下述SVM的泛化误差界
	\begin{equation}
		\mathbb{E}_{S \sim \mathcal{D}^m}[ R(h_S) ] \leqslant \mathbb{E}_{S \sim \mathcal{D}^{m+1}} \left[ \frac{N_{SV}(S)}{m+1} \right] , 
	\end{equation}
	其中$N_{SV}(S)$为$h_S$支持向量的个数。
\end{enumerate}

\begin{solution}
~\\(1) 该小问参考\cite{10.5555/572351}一书中对于\cite{Luntz1969OnEo}一文中首先提出的类似定理的证明。
\\令$Z_i$代表$(x_i,y_i)$，则由题可得:
	\begin{equation}\nonumber	
	\begin{aligned}
\mathbb{E}_{S \sim \mathcal{D}^m} \left[\hat{R}_{\text{LOO}}(\mathcal{A})\right] &= \mathbb{E}_{S \sim \mathcal{D}^m} \left[\frac{1}{m} \sum_{i=1}^m 1_{ h_{ S-\{x_i\} } (x_i) \neq y_i }\right] \\&= \int \frac{1}{m} \sum_{i=1}^m \left(1_{ h_{ S-\{x_i\} } (x_i) \neq y_i }\right) d\Pr(Z_1)d\Pr(Z_2)\dots d\Pr(Z_m) \\&= \frac{1}{m} \sum_{i=1}^m \int \left(1_{ h_{ S-\{x_i\} } (x_i) \neq y_i }\right) d\Pr(Z_1)d\Pr(Z_2)\dots d\Pr(Z_m)\\&= \frac{1}{m} \sum_{i=1}^m \int \left[\int\left(1_{ h_{ S-\{x_i\} } (x_i) \neq y_i }\right)d\Pr(Z_i)\right] d\Pr(Z_1)\dots d\Pr(Z_{i-1})\\&\quad\ d\Pr(Z_{i+1})\dots d\Pr(Z_m)\\& = \frac{1}{m} \sum_{i=1}^m \int R(h_{S'}) d\Pr(Z_1)\dots d\Pr(Z_{i-1})d\Pr(Z_{i+1})\dots d\Pr(Z_m)\\&= \int R(h_{S'}) d\Pr(Z_1)\dots d\Pr(Z_{n-1})\\&= \mathbb{E}_{S' \sim \mathcal{D}^{m-1}} [ R(h_{S'}) ]
	\end{aligned}
	\end{equation}
 (2) $h_{S-\{x\}}$可以将$x$正确分类，因为训练结果$h_{S-\{x\}} = h_S$。在$h_S$下，$x$不是支持向量，可以被正确分类，因此$h_{S-\{x\}}$下$x$也可以被正确分类。
 \\该结论的逆否命题为：若$h_{S-\{x\}}$不能正确分类$x$，则$x$不是$h_S$的支持向量。 
 \\(3) 根据(7)式可得:
 $$\mathbb{E}_{S' \sim \mathcal{D}^{m+1}} [\frac{1}{m+1} \sum_{i=1}^{m+1} 1_{ h_{ S'-\{x_i\} } (x_i) \neq y_i }] = \mathbb{E}_{S \sim \mathcal{D}^{m}} [ R(h_{S}) ] $$
 又由上题结论有:
 $$1_{ h_{ S'-\{x_i\} } (x_i) \neq y_i } \leqslant N_{SV}(S)$$
 $$\mathbb{E}_{S \sim \mathcal{D}^m}[ R(h_S) ] \leqslant \mathbb{E}_{S \sim \mathcal{D}^{m+1}} \left[ \frac{N_{SV}(S)}{m+1} \right]$$
 故得证。
\end{solution}

\section*{[20 pts] Problem 4 [NN in Practice]}

\textbf{请结合编程题指南进行理解}
\par 在训练神经网络之前，我们需要确定的是整个网络的结构，在确定结构后便可以输入数据进行端到端的学习过程。考虑一个简单的神经网络：输入是2维向量，隐藏层由2个隐层单元组成，输出层为1个输出单元，其中隐层单元和输出层单元的激活函数都是$Sigmoid$函数。请打开\textbf{main.py}程序并完成以下任务：
\begin{enumerate}[(1)]
	\item \textbf{[4 pts]} 请完成Sigmoid函数及其梯度函数的编写。
	\item \textbf{[2 pts]} 请完成MSE损失函数的编写。
	\item \textbf{[9 pts]} 请完成NeuralNetwork\_221()类中train函数的编写，其中包括向前传播(可参考predict函数)、梯度计算、更新参数三个部分。
	\item \textbf{[5 pts]} 请对测试集(test\_feature.csv)所提供的数据特征完成尽量准确的分类预测。
\end{enumerate}

\begin{solution}
~\\函数实现及相应预测结果见附件。
\end{solution}

\bibliographystyle{plain}
\bibliography{ref.bib}
\end{document}